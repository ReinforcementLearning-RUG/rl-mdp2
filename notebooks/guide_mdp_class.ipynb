{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to use the MDP class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e076dc0b7b167c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook briefly describes how to use the MDP class that is provided with the code.\n",
    "\n",
    "**The notebook format is used to give short examples, do not implement the algorithms inside a Jupyter notebook**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "815c82dea995fc30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl_mdp.mdp.reward_function import RewardFunction\n",
    "from rl_mdp.mdp.transition_function import TransitionFunction\n",
    "from rl_mdp.policy.policy import Policy\n",
    "from rl_mdp.mdp.mdp import MDP"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:35.970608Z",
     "start_time": "2024-09-12T10:58:35.853317Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that environments can be modelled as a Markov Decision Process (MDP), which is defines as a tuple $(\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ where\n",
    "* $\\mathcal{S}$ is the set of states.\n",
    "* $\\mathcal{A}$ is the set of actions.\n",
    "* $p : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$  is a transition function.\n",
    "* $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is a reward function.\n",
    "* $\\gamma \\in [0,1]$ is a discount factor.\n",
    "\n",
    "The ```mdp``` class can be used to implement a simple mdp with a discrete state and action space.\n",
    "\n",
    "For example, suppose we wanted to implement a simple mdp with 3 states and 2 actions:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a5a9d3f56c54a35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "states = [0, 1, 2]    # Set of states actions represented as a list of integers.\n",
    "actions = [0, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:37.568059Z",
     "start_time": "2024-09-12T10:58:37.566068Z"
    }
   },
   "id": "873c0b3cdf2cdd71",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we must specify the reward function, which should give a reward for every state action pair"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76894ffad03f9f2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Define rewards using a dictionary\n",
    "rewards = {\n",
    "    (0, 0): -1.0,           # state 0, action 0 gets reward -1.\n",
    "    (0, 1): -1.0,\n",
    "    (1, 0): 5.0,\n",
    "    (1, 1): -1.0,\n",
    "    (2, 0): -1.0,\n",
    "    (2, 1): 10.0\n",
    "}\n",
    "\n",
    "# Create the RewardFunction object\n",
    "reward_function = RewardFunction(rewards)\n",
    "\n",
    "# Now calling the reweard function\n",
    "print(reward_function(2, 1))           # This should return 10.0."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:40.513197Z",
     "start_time": "2024-09-12T10:58:40.510389Z"
    }
   },
   "id": "1c927da771efe570",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next lets specify the transition function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b62f295faa25e84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.8 0.1]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# Define transition probabilities using a dictionary\n",
    "transitions = {\n",
    "    (0, 0): np.array([0.7, 0.2, 0.1]),      # For state one, action one we get probability vector (0.7, 0.2, 0.1) representing the probability to transition to state 0, 1, 2 respectively.\n",
    "    (0, 1): np.array([0.1, 0.8, 0.1]),\n",
    "    (1, 0): np.array([0.4, 0.5, 0.1]),\n",
    "    (1, 1): np.array([0.3, 0.3, 0.4]),\n",
    "    (2, 0): np.array([0.9, 0.05, 0.05]),\n",
    "    (2, 1): np.array([0.2, 0.2, 0.6])\n",
    "}\n",
    "\n",
    "# Create the TransitionFunction object\n",
    "transition_function = TransitionFunction(transitions)\n",
    "\n",
    "# Example usage: Get the full transition probabilities array for state 0 and action 1\n",
    "print(transition_function(0, 1))  # Output: [0.1 0.8 0.1]\n",
    "print(transition_function(0, 1)[2])  # Probability P(S'=2|S=0, A=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:42.356174Z",
     "start_time": "2024-09-12T10:58:42.352582Z"
    }
   },
   "id": "38be099279bd79e2",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all you need to do is pass each component to the MDP class:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b6e2fcd93ad420b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create the MDP object\n",
    "mdp = MDP(states, actions, transition_function, reward_function, discount_factor=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:44.751860Z",
     "start_time": "2024-09-12T10:58:44.749778Z"
    }
   },
   "id": "268f72a22840e332",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability from state 0 to state 1 with action 1: 0.8\n",
      "Reward for taking action 1 in state 2: 10.0\n",
      "States: [0, 1, 2]\n",
      "Actions: [0, 1]\n",
      "Discount Factor: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Example usage of MDP functionalities:\n",
    "\n",
    "# Get transition probability of nex_state S'=1, given state S=0 and action A=1\n",
    "print(f\"Transition probability from state 0 to state 1 with action 1: {mdp.transition_prob(1, 0, 1)}\")\n",
    "\n",
    "# Get reward for taking action 1 in state 2\n",
    "print(f\"Reward for taking action 1 in state 2: {mdp.reward(2, 1)}\")\n",
    "\n",
    "# Get list of states\n",
    "print(f\"States: {mdp.states}\")\n",
    "\n",
    "# Get list of actions\n",
    "print(f\"Actions: {mdp.actions}\")\n",
    "\n",
    "# Get discount factor\n",
    "print(f\"Discount Factor: {mdp.discount_factor}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:46.073840Z",
     "start_time": "2024-09-12T10:58:46.071523Z"
    }
   },
   "id": "4204bde02869c883",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the assignment you will need to implement the `BellmanEquationSolver` and `DynamicProgrammingSolver` which should take a MDP as argument to their constructors and offer\n",
    "methods that return a policy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "330fd4324bf9da49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Policy class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10723a0330ea9af8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `Policy` class implements a basic stochastic policy which can give probabilities for each action given a state. You can also use it to implement a deterministic policy by passing an array which maps each state to an action."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba66c78d98b7f020"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of action 0 given state 0: 1.0\n",
      "Probability of action 1 given state 0: 0.0\n",
      "Probability of action 2 given state 0: 0.0\n",
      "____\n",
      "Probability of action 1 given state 1: 0.1\n"
     ]
    }
   ],
   "source": [
    "num_actions = 3\n",
    "policy_mapping = np.array([0, 2, 1])        # S=0 -> A=0, S=1 -> A=2, S=1 -> A=1\n",
    "\n",
    "policy = Policy(policy_mapping=policy_mapping, num_actions=num_actions)\n",
    "\n",
    "# Policy mapping implies deterministic policy so only one action gets probability 1.\n",
    "print(f\"Probability of action 0 given state 0: {policy.action_prob(0, 0)}\")\n",
    "print(f\"Probability of action 1 given state 0: {policy.action_prob(0, 1)}\")\n",
    "print(f\"Probability of action 2 given state 0: {policy.action_prob(0, 2)}\")\n",
    "print(\"____\")\n",
    "\n",
    "# You can also manually set the action probabilities for each state.\n",
    "policy.set_action_probabilities(1, [0.1, 0.1, 0.8])\n",
    "print(f\"Probability of action 1 given state 1: { policy.action_prob(1, 1)}\")\n",
    "# Output: Probability of action 1 given state 1: 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T10:58:48.667102Z",
     "start_time": "2024-09-12T10:58:48.663956Z"
    }
   },
   "id": "2adecfabcdbfe03b",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
